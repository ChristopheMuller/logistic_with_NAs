n=65000
d=NULL
corr=0.95
data_gen="LOG"
log_bias=0
lda_dist=NULL
missingness="MCAR"
alpha=NULL
n_train = 50000
score_fun_wrapper <- function(y_probs, y, preds){
return(calculate_score(y_probs, y, preds, score=SCORE))
}
all_files_name <- criteria_all_files(d, corr, data_gen, log_bias, lda_dist, missingness, alpha, n_train)
data_gens <- criteria_files(d, corr, data_gen, log_bias, lda_dist, missingness, alpha, n_train)
scores <- merge_data_criteria(all_files_name, data_gens, score_fun_wrapper)
# Plot
uncertainty_wrapper <- function(x){
return(compute_uncertainty(x, UNCERTAINTY))
}
methods <- names(scores)
score_means <- sapply(scores, mean)
score_uncertainty <- sapply(scores, uncertainty_wrapper)
df <- data.frame(methods, score_means, score_uncertainty)
ggplot(df, aes(x=methods, y=score_means, fill=methods)) +
geom_bar(stat="identity") +
geom_errorbar(aes(ymin=score_means-score_uncertainty, ymax=score_means+score_uncertainty), width=.2,
position=position_dodge(.9)) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(x="Method", y=paste0(SCORE, " (", UNCERTAINTY, ")")) +
scale_fill_brewer(palette="Set3")
print("Number of scores for each method:")
for (met in methods) {
print(paste0(met, ": ",length(scores[[met]])))
}
source("script_utils.R")
library("ggplot2")
SCORE = "MAE"
UNCERTAINTY = "se"
plot_scores_by_criterion("corr", n_train=50000, data_gen="LOG", SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
knitr::opts_chunk$set(echo = TRUE)
library("ggplot2")
source("script_utils.R")
library("ggplot2")
knitr::opts_chunk$set(echo = TRUE)
source("script_utils.R")
library("ggplot2")
```{r pressure, echo=TRUE}
SCORE <- "MAE"
UNCERTAINTY <- "se"
knitr::opts_chunk$set(echo = TRUE)
source("script_utils.R")
library("ggplot2")
SCORE <- "MAE"
UNCERTAINTY <- "se"
plot_scores_by_criterion("data_gen", n_train=50000, SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
plot_scores_by_criterion("missingness", n_train=50000, SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
plot_scores_by_criterion("alpha", n_train=50000, missingness = "MCAR", SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
plot_scores_by_criterion("alpha", n_train=50000, missingness = "MCAR", SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
plot_scores_by_criterion("d", n_train=50000, SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
source("script_utils.R")
library("ggplot2")
SCORE = "MAE"
UNCERTAINTY = "se"
# SET-UP
n=65000
d=NULL
corr=0.95
data_gen="LOG"
log_bias=0
lda_dist=NULL
missingness="MCAR"
alpha=NULL
n_train = 50000
score_fun_wrapper <- function(y_probs, y, preds){
return(calculate_score(y_probs, y, preds, score=SCORE))
}
all_files_name <- criteria_all_files(d, corr, data_gen, log_bias, lda_dist, missingness, alpha, n_train)
data_gens <- criteria_files(d, corr, data_gen, log_bias, lda_dist, missingness, alpha, n_train)
scores <- merge_data_criteria(all_files_name, data_gens, score_fun_wrapper)
# Plot
uncertainty_wrapper <- function(x){
return(compute_uncertainty(x, UNCERTAINTY))
}
methods <- names(scores)
score_means <- sapply(scores, mean)
score_uncertainty <- sapply(scores, uncertainty_wrapper)
df <- data.frame(methods, score_means, score_uncertainty)
ggplot(df, aes(x=methods, y=score_means, fill=methods)) +
geom_bar(stat="identity") +
geom_errorbar(aes(ymin=score_means-score_uncertainty, ymax=score_means+score_uncertainty), width=.2,
position=position_dodge(.9)) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(x="Method", y=paste0(SCORE, " (", UNCERTAINTY, ")")) +
scale_fill_brewer(palette="Set3")
print("Number of scores for each method:")
for (met in methods) {
print(paste0(met, ": ",length(scores[[met]])))
}
source("script_utils.R")
library("ggplot2")
SCORE = "MAE"
UNCERTAINTY = "se"
# SET-UP
n=65000
d=NULL
corr=0.95
data_gen="LOG"
log_bias=NULL
lda_dist=NULL
missingness="MCAR"
alpha=NULL
n_train = 50000
score_fun_wrapper <- function(y_probs, y, preds){
return(calculate_score(y_probs, y, preds, score=SCORE))
}
all_files_name <- criteria_all_files(d, corr, data_gen, log_bias, lda_dist, missingness, alpha, n_train)
data_gens <- criteria_files(d, corr, data_gen, log_bias, lda_dist, missingness, alpha, n_train)
scores <- merge_data_criteria(all_files_name, data_gens, score_fun_wrapper)
# Plot
uncertainty_wrapper <- function(x){
return(compute_uncertainty(x, UNCERTAINTY))
}
methods <- names(scores)
score_means <- sapply(scores, mean)
score_uncertainty <- sapply(scores, uncertainty_wrapper)
df <- data.frame(methods, score_means, score_uncertainty)
ggplot(df, aes(x=methods, y=score_means, fill=methods)) +
geom_bar(stat="identity") +
geom_errorbar(aes(ymin=score_means-score_uncertainty, ymax=score_means+score_uncertainty), width=.2,
position=position_dodge(.9)) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(x="Method", y=paste0(SCORE, " (", UNCERTAINTY, ")")) +
scale_fill_brewer(palette="Set3")
print("Number of scores for each method:")
for (met in methods) {
print(paste0(met, ": ",length(scores[[met]])))
}
plot_scores_by_criterion("corr", n_train=50000, SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
plot_scores_by_criterion("corr", n_train=50000, SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
knitr::opts_chunk$set(echo = TRUE)
source("script_utils.R")
library("ggplot2")
SCORE <- "CLASS"
UNCERTAINTY <- "se"
plot_scores_by_criterion("data_gen", n_train=50000, SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
plot_scores_by_criterion("d", n_train=50000, SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
plot_scores_by_criterion("missingness", n_train=50000, SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
plot_scores_by_criterion("alpha", n_train=50000, missingness = "MCAR", SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
plot_scores_by_criterion("corr", n_train=50000, SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
knitr::opts_chunk$set(echo = TRUE)
source("script_utils.R")
library("ggplot2")
SCORE <- "MAE"
UNCERTAINTY <- "se"
plot_scores_by_criterion("data_gen", n_train=50000, SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
plot_scores_by_criterion("d", n_train=50000, SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
plot_scores_by_criterion("missingness", n_train=50000, SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
plot_scores_by_criterion("alpha", n_train=50000, missingness = "MCAR", SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
plot_scores_by_criterion("corr", n_train=50000, SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
10 * 4 * 3 * 3 * 3
plot_evolution_duplicates <- function(Ks, n, d, corr, data_gen, log_bias, lda_dist, missingness, alpha, SCORE, UNCERTAINTY){
data_list <- list()
# Create a data frame to store MAE values
score_data <- data.frame()
for (k in ks){
for (n_training in N_trains){
# Find the files
files <- find_files_data_gen("data/results_new/", n_training=n_training, k=k, n=n, d=d,
corr=corr, data_gen=data_gen, missingness=missingness, alpha=alpha, log_bias=log_bias, lda_dist=lda_dist)
print(files)
df <- merge_data_results(files, y_probs=TRUE)
data_list[[as.character(n_training)]] <- df
}
# Iterate through each data frame and calculate MAE for each model
for (n_train in N_trains) {
df <- data_list[[as.character(n_train)]]
for (model_name in colnames(df)[3:ncol(df)]) {
score <- calculate_score(df$y_probs, df$y, df[, model_name], score=SCORE)
score_data <- rbind(score_data, data.frame(
n_training = n_train,
model_name = model_name,
score = score
))
}
}
print(k)
}
score_data <- score_data %>%
group_by(n_training, model_name) %>%
summarise(
mean_score = mean(score),
sd_score = compute_uncertainty(score, UNCERTAINTY),
n_scores = n()
)
ggplot(score_data, aes(x = n_training, y = mean_score, color = model_name, linetype = model_name)) +
geom_line(size = 1) +
geom_ribbon(aes(ymin = mean_score - sd_score, ymax = mean_score + sd_score, fill = model_name),
alpha = 0.2, color = NA) +
labs(
x = "Number of Training Points",
y = SCORE,
title = "Score vs. Training Size (Aggregated over k)"
) +
scale_x_log10() +
scale_color_manual(values = c(
"LDA_0" = "blue", "LDA_ICE" = "blue", "LDA_MCAR" = "blue", "LDA_Mean" = "orange",
"LDA_Opt" = "orange", "LDA_PBP" = "orange",
"LOG_0" = "red", "LOG_ICE_" = "red", "LOG_PBP" = "red",
"SAEM" = "purple", "LOG_ICEY" = "purple"
)) +
scale_fill_manual(values = c(
"LDA_0" = "blue", "LDA_ICE" = "blue", "LDA_MCAR" = "blue", "LDA_Mean" = "orange",
"LDA_Opt" = "orange", "LDA_PBP" = "orange",
"LOG_0" = "red", "LOG_ICE_" = "red", "LOG_PBP" = "red",
"SAEM" = "purple", "LOG_ICEY" = "purple"
)) +
scale_linetype_manual(values = c(
"LDA_0" = "solid", "LDA_ICE" = "dashed", "LDA_MCAR" = "dotted", "LDA_Mean" = "solid",
"LDA_Opt" = "dashed", "LDA_PBP" = "dotted",
"LOG_0" = "solid", "LOG_ICE_" = "dashed", "LOG_PBP" = "dotted",
"SAEM" = "solid", "LOG_ICEY" = "dashed"
)) +
theme_minimal()
# count number of scores for each method
print("Number of scores for each method:")
score_data[["n_scores"]] <- as.numeric(score_data[["n_scores"]])
print(score_data[,c("model_name","n_training" ,"n_scores")], n=100)
}
plot_evolution_duplicates(ks, n, d, corr, data_gen, log_bias, lda_dist, missingness, alpha, SCORE, UNCERTAINTY)
# SET-UP
ks=0:9
n=65000
d=3
corr=0.95
data_gen="LOG"
log_bias=0
lda_dist=0.65
missingness="MCAR"
alpha=0.2
plot_evolution_duplicates(ks, n, d, corr, data_gen, log_bias, lda_dist, missingness, alpha, SCORE, UNCERTAINTY)
plot_evolution_duplicates <- function(Ks, n, d, corr, data_gen, log_bias, lda_dist, missingness, alpha, SCORE, UNCERTAINTY){
data_list <- list()
# Create a data frame to store MAE values
score_data <- data.frame()
for (k in ks){
for (n_training in N_trains){
# Find the files
files <- find_files_data_gen("data/results_new/", n_training=n_training, k=k, n=n, d=d,
corr=corr, data_gen=data_gen, missingness=missingness, alpha=alpha, log_bias=log_bias, lda_dist=lda_dist)
df <- merge_data_results(files, y_probs=TRUE)
data_list[[as.character(n_training)]] <- df
}
# Iterate through each data frame and calculate MAE for each model
for (n_train in N_trains) {
df <- data_list[[as.character(n_train)]]
for (model_name in colnames(df)[3:ncol(df)]) {
score <- calculate_score(df$y_probs, df$y, df[, model_name], score=SCORE)
score_data <- rbind(score_data, data.frame(
n_training = n_train,
model_name = model_name,
score = score
))
}
}
print(k)
}
score_data <- score_data %>%
group_by(n_training, model_name) %>%
summarise(
mean_score = mean(score),
sd_score = compute_uncertainty(score, UNCERTAINTY),
n_scores = n()
)
ggplot(score_data, aes(x = n_training, y = mean_score, color = model_name, linetype = model_name)) +
geom_line(size = 1) +
geom_ribbon(aes(ymin = mean_score - sd_score, ymax = mean_score + sd_score, fill = model_name),
alpha = 0.2, color = NA) +
labs(
x = "Number of Training Points",
y = SCORE,
title = "Score vs. Training Size (Aggregated over k)"
) +
scale_x_log10() +
scale_color_manual(values = c(
"LDA_0" = "blue", "LDA_ICE" = "blue", "LDA_MCAR" = "blue", "LDA_Mean" = "orange",
"LDA_Opt" = "orange", "LDA_PBP" = "orange",
"LOG_0" = "red", "LOG_ICE_" = "red", "LOG_PBP" = "red",
"SAEM" = "purple", "LOG_ICEY" = "purple"
)) +
scale_fill_manual(values = c(
"LDA_0" = "blue", "LDA_ICE" = "blue", "LDA_MCAR" = "blue", "LDA_Mean" = "orange",
"LDA_Opt" = "orange", "LDA_PBP" = "orange",
"LOG_0" = "red", "LOG_ICE_" = "red", "LOG_PBP" = "red",
"SAEM" = "purple", "LOG_ICEY" = "purple"
)) +
scale_linetype_manual(values = c(
"LDA_0" = "solid", "LDA_ICE" = "dashed", "LDA_MCAR" = "dotted", "LDA_Mean" = "solid",
"LDA_Opt" = "dashed", "LDA_PBP" = "dotted",
"LOG_0" = "solid", "LOG_ICE_" = "dashed", "LOG_PBP" = "dotted",
"SAEM" = "solid", "LOG_ICEY" = "dashed"
)) +
theme_minimal()
# count number of scores for each method
# print("Number of scores for each method:")
# score_data[["n_scores"]] <- as.numeric(score_data[["n_scores"]])
# print(score_data[,c("model_name","n_training" ,"n_scores")], n=100)
}
plot_evolution_duplicates(ks, n, d, corr, data_gen, log_bias, lda_dist, missingness, alpha, SCORE, UNCERTAINTY)
SCORE
SCORE = "CLASS"
UNCERTAINTY = "se"
# SET-UP
ks=0:9
n=65000
d=3
corr=0.95
data_gen="LOG"
log_bias=0
lda_dist=0.65
missingness="MCAR"
alpha=0.2
plot_evolution_duplicates(ks, n, d, corr, data_gen, log_bias, lda_dist, missingness, alpha, SCORE, UNCERTAINTY)
SCORE = "MAE"
UNCERTAINTY = "se"
# SET-UP
ks=0:9
n=65000
d=3
corr=0.95
data_gen="LOG"
log_bias=0
lda_dist=0.65
missingness="MCAR"
alpha=0.2
plot_evolution_duplicates(ks, n, d, corr, data_gen, log_bias, lda_dist, missingness, alpha, SCORE, UNCERTAINTY)
plot_evolution_duplicates <- function(Ks, n, d, corr, data_gen, log_bias, lda_dist, missingness, alpha, SCORE, UNCERTAINTY){
data_list <- list()
# Create a data frame to store MAE values
score_data <- data.frame()
for (k in ks){
for (n_training in N_trains){
# Find the files
files <- find_files_data_gen("data/results_new/", n_training=n_training, k=k, n=n, d=d,
corr=corr, data_gen=data_gen, missingness=missingness, alpha=alpha, log_bias=log_bias, lda_dist=lda_dist)
df <- merge_data_results(files, y_probs=TRUE)
data_list[[as.character(n_training)]] <- df
}
# Iterate through each data frame and calculate MAE for each model
for (n_train in N_trains) {
df <- data_list[[as.character(n_train)]]
for (model_name in colnames(df)[3:ncol(df)]) {
score <- calculate_score(df$y_probs, df$y, df[, model_name], score=SCORE)
score_data <- rbind(score_data, data.frame(
n_training = n_train,
model_name = model_name,
score = score
))
}
}
print(k)
}
score_data <- score_data %>%
group_by(n_training, model_name) %>%
summarise(
mean_score = mean(score),
sd_score = compute_uncertainty(score, UNCERTAINTY),
n_scores = n()
)
ggplot(score_data, aes(x = n_training, y = mean_score, color = model_name, linetype = model_name)) +
geom_line(size = 1) +
geom_ribbon(aes(ymin = mean_score - sd_score, ymax = mean_score + sd_score, fill = model_name),
alpha = 0.2, color = NA) +
labs(
x = "Number of Training Points",
y = paste0(SCORE, " (", UNCERTAINTY, ")"),
title = "Score vs. Training Size (Aggregated over k)"
) +
scale_x_log10() +
scale_color_manual(values = c(
"LDA_0" = "blue", "LDA_ICE" = "blue", "LDA_MCAR" = "blue", "LDA_Mean" = "orange",
"LDA_Opt" = "orange", "LDA_PBP" = "orange",
"LOG_0" = "red", "LOG_ICE_" = "red", "LOG_PBP" = "red",
"SAEM" = "purple", "LOG_ICEY" = "purple"
)) +
scale_fill_manual(values = c(
"LDA_0" = "blue", "LDA_ICE" = "blue", "LDA_MCAR" = "blue", "LDA_Mean" = "orange",
"LDA_Opt" = "orange", "LDA_PBP" = "orange",
"LOG_0" = "red", "LOG_ICE_" = "red", "LOG_PBP" = "red",
"SAEM" = "purple", "LOG_ICEY" = "purple"
)) +
scale_linetype_manual(values = c(
"LDA_0" = "solid", "LDA_ICE" = "dashed", "LDA_MCAR" = "dotted", "LDA_Mean" = "solid",
"LDA_Opt" = "dashed", "LDA_PBP" = "dotted",
"LOG_0" = "solid", "LOG_ICE_" = "dashed", "LOG_PBP" = "dotted",
"SAEM" = "solid", "LOG_ICEY" = "dashed"
)) +
theme_minimal()
# count number of scores for each method
# print("Number of scores for each method:")
# score_data[["n_scores"]] <- as.numeric(score_data[["n_scores"]])
# print(score_data[,c("model_name","n_training" ,"n_scores")], n=100)
}
plot_evolution_duplicates(ks, n, d, corr, data_gen, log_bias, lda_dist, missingness, alpha, SCORE, UNCERTAINTY)
knitr::opts_chunk$set(echo = TRUE)
source("script_utils.R")
library("ggplot2")
SCORE <- "MAE"
UNCERTAINTY <- "se"
ks=0:9
n=65000
d=3
corr=0.95
data_gen="LOG"
log_bias=0
lda_dist=0.65
missingness="MCAR"
alpha=0.2
plot_evolution_duplicates(ks, n, d, corr, data_gen, log_bias, lda_dist, missingness, alpha, SCORE, UNCERTAINTY)
plot_scores_by_criterion("d", n_train=50000, SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
plot_scores_by_criterion("missingness", n_train=50000, SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
knitr::opts_chunk$set(echo = TRUE)
source("script_utils.R")
library("ggplot2")
SCORE <- "MAE"
UNCERTAINTY <- "se"
ks=0:9
n=65000
d=3
corr=0.95
data_gen="LOG"
log_bias=0
lda_dist=0.65
missingness="MCAR"
alpha=0.2
plot_evolution_duplicates(ks, n, d, corr, data_gen, log_bias, lda_dist, missingness, alpha, SCORE, UNCERTAINTY)
plot_scores_by_criterion("d", n_train=50000, SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
plot_scores_by_criterion("missingness", n_train=50000, SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
plot_scores_by_criterion("alpha", n_train=50000, missingness = "MCAR", SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
plot_scores_by_criterion("corr", n_train=50000, SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
plot_evolution_duplicates(ks, n, d, corr, data_gen, log_bias, lda_dist, missingness, alpha, SCORE, UNCERTAINTY)
knitr::opts_chunk$set(echo = TRUE)
source("script_utils.R")
library("ggplot2")
SCORE <- "MAE"
UNCERTAINTY <- "se"
ks=0:9
n=65000
d=3
corr=0.95
data_gen="LOG"
log_bias=0
lda_dist=0.65
missingness="MCAR"
alpha=0.2
plot_evolution_duplicates(ks, n, d, corr, data_gen, log_bias, lda_dist, missingness, alpha, SCORE, UNCERTAINTY)
plot_scores_by_criterion("data_gen", n_train=50000, SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
plot_scores_by_criterion("d", n_train=50000, SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
plot_scores_by_criterion("missingness", n_train=50000, SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
plot_scores_by_criterion("alpha", n_train=50000, missingness = "MCAR", SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
plot_scores_by_criterion("corr", n_train=50000, SCORE=SCORE, UNCERTAINTY=UNCERTAINTY)
source("script_utils.R")
library("ggplot2")
library("dplyr")
SCORE = "MAE"
UNCERTAINTY = "se"
n_trains <- c(1000,5000,20000,50000)
# SET-UP
ks=0:9
n=65000
d=3
corr=0.95
data_gen="LOG"
log_bias=0
lda_dist=0.65
missingness="MCAR"
alpha=0.2
plot_evolution_duplicates(ks, n, d, corr, data_gen, log_bias, lda_dist, missingness, alpha, n_trains, SCORE, UNCERTAINTY)
source("script_utils.R")
library(triptych)
# SET-UP
k=0
n=65000
d=3
corr=0
data_gen="LOG"
log_bias=0
lda_dist=NULL
missingness="MCAR"
alpha=0.2
n_training=1000
# Find the files
files <- find_files_data_gen("data/results_new/", n_training=n_training, k=k, n=n, d=d,
corr=corr, data_gen=data_gen, missingness=missingness, alpha=alpha, log_bias=log_bias, lda_dist=lda_dist)
df <- merge_data_results(files)
tr <- triptych(df)
names(df)
dplyr::slice(tr, 1,3,8,9) |>
add_consistency(level = 0.9, method = "resampling_Bernoulli", n_boot = 20) |>
autoplot() &
ggplot2::guides(colour = ggplot2::guide_legend("Forecast"))
library(ggplot2)
md <- mcbdsc(df)
# autoplot(md) +
#   coord_cartesian(xlim = c(0, 0.010), ylim = c(0.05,0.062))
estimates(md)
# sort estimates by column "mean_score"
estimates(md) |>
dplyr::arrange(mean_score) |>
head(20)
