M_test <- data_test$f[["M"]]
y_probs_test <- data_test$f[["y_probs"]]
y_test <- data_test$f[["y"]]
data_bayes <- np$load(file.path("data", exp, "bayes_data", "LOG_n315000_d3_corr065_prcNA035_prop105_rep0.npz"))
y_bayes <- data_bayes$f[["y_probs_bayes"]]
# Load the true beta
true_beta <- c(0, 1.62434536, -0.61175641, -0.52817175)
true_sigma <- toeplitz(c(1, 0.65, 0.65^2))
true_mu <- c(0, 0, 0)
n_train <- 1000
X_train <- X_obs[1:n_train,]
M_train <- M[1:n_train,]
print(sum(M_train)/length(M_train))
y_train <- y[1:n_train]
data_train <- as.data.frame(X_train)
data_train$y <- y_train
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=10, maxruns=20, k1=2, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
true_mu
model$mu.X
model$trace$mu
model$trace$beta
all_predictions <- list()
for (i in 1:1000){
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=10, maxruns=20, k1=2, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=50000)
all_predictions[[i]] <- y.hat
print(i)
}
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=10, maxruns=20, k1=2, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=500)
mean(abs(y.hat[,1] - y_bayes[1:100]))
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=10, maxruns=20, k1=2, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=10, maxruns=20, k1=1, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=500)
mean(abs(y.hat[,1] - y_bayes[1:100]))
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=10, maxruns=20, k1=1, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=500)
mean(abs(y.hat[,1] - y_bayes[1:100]))
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=10, maxruns=20, k1=1, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=500)
mean(abs(y.hat[,1] - y_bayes[1:100]))
for (i in 1:10){
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=10, maxruns=20, k1=1, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=500)
all_predictions[[i]] <- y.hat
print(i)
}
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=10, maxruns=20, k1=2, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=500)
mean(abs(y.hat[,1] - y_bayes[1:100]))
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=10, maxruns=20, k1=2, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=500)
mean(abs(y.hat[,1] - y_bayes[1:100]))
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=10, maxruns=20, k1=2, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=500)
mean(abs(y.hat[,1] - y_bayes[1:100]))
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=10, maxruns=20, k1=2, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=500)
mean(abs(y.hat[,1] - y_bayes[1:100]))
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=10, maxruns=20, k1=2, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=500)
mean(abs(y.hat[,1] - y_bayes[1:100]))
all_predictions <- list()
for (i in 1:10){
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=10, maxruns=20, k1=2, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=500)
all_predictions[[i]] <- y.hat
print(i)
}
for (i in 1:3){
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=10, maxruns=20, k1=2, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=500)
all_predictions[[i]] <- y.hat
print(i)
}
y.hat <- do.call(rbind, all_predictions)
y.hat <- colMeans(y.hat)
mean(abs(y.hat[,1] - y_bayes[1:100]))
y.hat
y.hat <- do.call(rbind, all_predictions)
y.hat
y.hat <- do.call(cbind, all_predictions)
y.hat
y.hat <- colMeans(y.hat)
y.hat
y.hat <- do.call(cbind, all_predictions)
y.hat <- rowMeans(y.hat)
y.hat
mean(abs(y.hat[,1] - y_bayes[1:100]))
mean(abs(y.hat - y_bayes[1:100]))
for (i in 1:10){
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=10, maxruns=20, k1=2, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=500)
all_predictions[[i]] <- y.hat
print(i)
}
y.hat <- do.call(cbind, all_predictions)
y.hat <- rowMeans(y.hat)
mean(abs(y.hat - y_bayes[1:100]))
all_predictions <- list()
for (i in 1:100){
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=5, maxruns=20, k1=2, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=100)
all_predictions[[i]] <- y.hat
print(i)
}
all_predictions <- list()
for (i in 1:100){
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=5, maxruns=20, k1=2, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=500)
all_predictions[[i]] <- y.hat
print(i)
}
y.hat <- do.call(cbind, all_predictions)
y.hat <- rowMeans(y.hat)
mean(abs(y.hat - y_bayes[1:100]))
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=5, maxruns=20, k1=2, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
mean(abs(y.hat[,1] - y_bayes[1:100]))
mean(abs(y.hat[,1] - y_bayes[1:100]))
y.hat
mean(abs(y.hat - y_bayes[1:100]))
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=5, maxruns=20, k1=2, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=50000)
mean(abs(y.hat - y_bayes[1:100]))
mean(abs(y.hat[,1] - y_bayes[1:100]))
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=5, maxruns=20, k1=2, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=50000)
mean(abs(y.hat[,1] - y_bayes[1:100]))
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=5, maxruns=20, k1=2, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=500)
mean(abs(y.hat[,1] - y_bayes[1:100]))
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=5, maxruns=20, k1=2, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=500)
mean(abs(y.hat[,1] - y_bayes[1:100]))
results <- readRDS("~/INRIA/R_scripts/logistic_with_NAs/data/ExpC/SAEM_deepdive_tau08_500mcmc.RDS")
true_beta <- c(0, 1.62434536, -0.61175641, -0.52817175)
true_sigma <- toeplitz(c(1, 0.65, 0.65^2))
true_mu <- c(0, 0, 0)
running_time <- unlist(results$runnint_time)
training_sizes <- as.numeric(names(running_time))
plot(training_sizes, running_time, type="b", xlab="Training size", ylab="Running time (s)", main="Running time vs Training size", log="x")
(max_time <- max(running_time) / 60)
beta_estimated <- results$beta_estimated
beta_estimated <- do.call(rbind, beta_estimated)
beta_estimated <- t(beta_estimated)
plot(true_beta, col="red", type="b", xlab="Beta index", ylab="Beta value", main="Beta estimation")
lines(beta_estimated[,1], col="blue", type="b")
lines(beta_estimated[,2], col="blue", type="b")
lines(beta_estimated[,3], col="blue", type="b")
lines(beta_estimated[,4], col="blue", type="b")
lines(beta_estimated[,5], col="blue", type="b")
lines(beta_estimated[,6], col="blue", type="b")
lines(beta_estimated[,7], col="blue", type="b")
lines(beta_estimated[,8], col="blue", type="b")
lines(beta_estimated[,9], col="blue", type="b")
mean_squared_errors <- apply(beta_estimated, 2, function(x) mean((x - true_beta)^2))
plot(training_sizes, mean_squared_errors, type="b", xlab="Beta index", ylab="Mean squared error", main="Mean squared error for beta estimation", log="x")
lines(x=training_sizes, rep(0, length(training_sizes)), col="red")
mean_abs_errors <- apply(beta_estimated, 2, function(x) mean(abs(x - true_beta)))
plot(training_sizes, mean_abs_errors, type="b", xlab="Beta index", ylab="Mean absolute error", main="Mean absolute error for beta estimation", log="x")
lines(x=training_sizes, rep(0, length(training_sizes)), col="red")
mu_estimated <- results$mu_estimated
mu_estimated <- do.call(rbind, mu_estimated)
mu_estimated <- t(mu_estimated)
plot(true_mu, col="red", type="b", xlab="Mu index", ylab="Mu value", main="Mu estimation")
lines(mu_estimated[,1], col="blue", type="b")
lines(mu_estimated[,2], col="blue", type="b")
lines(mu_estimated[,3], col="blue", type="b")
lines(mu_estimated[,4], col="blue", type="b")
lines(mu_estimated[,5], col="blue", type="b")
lines(mu_estimated[,6], col="blue", type="b")
lines(mu_estimated[,7], col="blue", type="b")
lines(mu_estimated[,8], col="blue", type="b")
lines(mu_estimated[,9], col="blue", type="b")
mean_squared_errors <- apply(mu_estimated, 2, function(x) mean((x - true_mu)^2))
plot(training_sizes, mean_squared_errors, type="b", xlab="Mu index", ylab="Mean squared error", main="Mean squared error for mu estimation", log="x")
lines(x=training_sizes, rep(0, length(training_sizes)), col="red")
mean_abs_errors <- apply(mu_estimated, 2, function(x) mean(abs(x - true_mu)))
plot(training_sizes, mean_abs_errors, type="b", xlab="Mu index", ylab="Mean absolute error", main="Mean absolute error for mu estimation", log="x")
lines(x=training_sizes, rep(0, length(training_sizes)), col="red")
sigma_estimated_all <-results$sigma_estimated
# restructure to get vector of S11, S22, S33, S12, S23, S13
sigma_estimated <- lapply(sigma_estimated_all, function(x) c(x[1,1], x[2,2], x[3,3], x[1,2], x[2,3], x[1,3]))
sigma_estimated <- do.call(rbind, sigma_estimated)
sigma_estimated <- t(sigma_estimated)
true_sigma_vec <- c(1, 1, 1, 0.65, 0.65, 0.65**2)
plot(true_sigma_vec, col="red", type="b", xlab="Sigma index", ylab="Sigma value", main="Sigma estimation")
lines(sigma_estimated[,1], col="blue", type="b")
lines(sigma_estimated[,2], col="blue", type="b")
lines(sigma_estimated[,3], col="blue", type="b")
lines(sigma_estimated[,4], col="blue", type="b")
lines(sigma_estimated[,5], col="blue", type="b")
lines(sigma_estimated[,6], col="blue", type="b")
lines(sigma_estimated[,7], col="blue", type="b")
lines(sigma_estimated[,8], col="blue", type="b")
lines(sigma_estimated[,9], col="blue", type="b")
mean_squared_errors <- apply(sigma_estimated, 2, function(x) mean((x - true_sigma_vec)^2))
plot(training_sizes, mean_squared_errors, type="b", xlab="Sigma index", ylab="Mean squared error", main="Mean squared error for sigma estimation", log="x")
lines(x=training_sizes, rep(0, length(training_sizes)), col="red")
mean_abs_errors <- apply(sigma_estimated, 2, function(x) mean(abs(x - true_sigma_vec)))
plot(training_sizes, mean_abs_errors, type="b", xlab="Sigma index", ylab="Mean absolute error", main="Mean absolute error for sigma estimation", log="x")
lines(x=training_sizes, rep(0, length(training_sizes)), col="red")
Frobenius_norm_fun <- function(x, y) sqrt(sum((x - y)^2))
frobenius_norms <- c()
for (train_size in training_sizes){
frobenius_norms <- c(frobenius_norms, Frobenius_norm_fun(sigma_estimated_all[[as.character(train_size)]], true_sigma))
}
plot(training_sizes, frobenius_norms, type="b", xlab="Training size", ylab="Frobenius norm", main="Frobenius norm for sigma estimation", log="x")
lines(x=training_sizes, rep(0, length(training_sizes)), col="red")
library(reticulate)
library(dplyr)
library(stringr)
source("methods_in_R.R")
exp <- "ExpC"
training_sizes <- c(100, 500, 1000, 5000, 10000, 50000, 100000, 200000, 300000)
test_size <- 15000
np <- import("numpy")
all_predictions <- list()
for (i in 1:100){
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=5, maxruns=20, k1=2, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=500)
all_predictions[[i]] <- y.hat
print(i)
}
data_train <- as.data.frame(X_train)
data_train$y <- y_train
X_train <- X_obs[1:n_train,]
M_train <- M[1:n_train,]
print(sum(M_train)/length(M_train))
exp <- "ExpC"
source("methods_in_R.R")
exp <- "ExpC"
training_sizes <- c(100, 500, 1000, 5000, 10000, 50000, 100000, 200000, 300000)
test_size <- 15000
np <- import("numpy")
data <- np$load(file.path("data", exp, "original_data", "LOG_n315000_d3_corr065_prcNA035_prop105_rep0.npz"))
X_obs <- data$f[["X_obs"]]
M <- data$f[["M"]]
y <- data$f[["y"]]
data_test <- np$load(file.path("data", exp, "test_data", "LOG_n315000_d3_corr065_prcNA035_prop105_rep0.npz"))
X_test <- data_test$f[["X_obs"]]
M_test <- data_test$f[["M"]]
y_probs_test <- data_test$f[["y_probs"]]
y_test <- data_test$f[["y"]]
data_bayes <- np$load(file.path("data", exp, "bayes_data", "LOG_n315000_d3_corr065_prcNA035_prop105_rep0.npz"))
y_bayes <- data_bayes$f[["y_probs_bayes"]]
true_beta <- c(0, 1.62434536, -0.61175641, -0.52817175)
true_sigma <- toeplitz(c(1, 0.65, 0.65^2))
true_mu <- c(0, 0, 0)
n_train <- 1000
X_train <- X_obs[1:n_train,]
M_train <- M[1:n_train,]
print(sum(M_train)/length(M_train))
y_train <- y[1:n_train]
data_train <- as.data.frame(X_train)
data_train$y <- y_train
all_predictions <- list()
for (i in 1:1){
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=5, maxruns=20, k1=2, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=500)
all_predictions[[i]] <- y.hat
print(i)
}
for (i in 1:1){
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=5, maxruns=20, k1=1, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=500)
all_predictions[[i]] <- y.hat
print(i)
}
y.hat <- do.call(cbind, all_predictions)
y.hat <- rowMeans(y.hat)
mean(abs(y.hat - y_bayes[1:100]))
mean(abs(y.hat[,1] - y_bayes[1:100]))
library(ggplot2)
library(dplyr)
library(tidyr)
# Number of iterations
num_iter <- length(model$trace$X.iter)
# Compute mean at each iteration
mean_evolution <- sapply(1:num_iter, function(k) {
colMeans(model$trace$X.iter[[k]])  # Compute column means
})
# Convert to a tidy dataframe for ggplot
df_plot <- data.frame(t(mean_evolution)) %>%
mutate(iteration = 1:num_iter) %>%
pivot_longer(cols = -iteration, names_to = "variable", values_to = "mean")
# Plot the evolution
ggplot(df_plot, aes(x = iteration, y = mean, color = variable)) +
geom_line() +
labs(title = "Evolution of Column Means Over Iterations",
x = "Iteration",
y = "Mean Value") +
theme_minimal()
library(ggplot2)
library(dplyr)
library(tidyr)
# Number of iterations
num_iter <- length(model$trace$X.iter)
# Compute covariance at each iteration
cov_evolution <- sapply(1:num_iter, function(k) {
Sigma <- cov(model$trace$X.iter[[k]])  # Compute covariance matrix
c(Sigma[1,1], Sigma[2,2], Sigma[3,3], Sigma[1,2], Sigma[2,3], Sigma[1,3])  # Extract elements
})
# Convert to a tidy dataframe for ggplot
df_cov <- data.frame(t(cov_evolution)) %>%
mutate(iteration = 1:num_iter) %>%
pivot_longer(cols = -iteration, names_to = "cov_element", values_to = "value")
# Rename elements for readability
df_cov$cov_element <- factor(df_cov$cov_element,
labels = c("Sigma11", "Sigma22", "Sigma33", "Sigma12", "Sigma23", "Sigma13"))
# Plot the evolution
ggplot(df_cov, aes(x = iteration, y = value, color = cov_element)) +
geom_line() +
labs(title = "Evolution of Covariance Matrix Elements",
x = "Iteration",
y = "Value") +
theme_minimal()
num_iter <- length(model$trace$X.iter)
# Compute covariance at each iteration
cov_evolution <- sapply(1:num_iter, function(k) {
Sigma <- cov(model$trace$X.iter[[k]])  # Compute covariance matrix
c(Sigma[1,1], Sigma[2,2], Sigma[3,3], Sigma[1,2], Sigma[2,3], Sigma[1,3])  # Extract elements
})
# Convert to a tidy dataframe for ggplot
df_cov <- data.frame(t(cov_evolution)) %>%
mutate(iteration = 1:num_iter) %>%
pivot_longer(cols = -iteration, names_to = "cov_element", values_to = "value")
# Rename elements for readability
df_cov$cov_element <- factor(df_cov$cov_element,
labels = c("Sigma11", "Sigma22", "Sigma33", "Sigma12", "Sigma23", "Sigma13"))
# Plot the evolution
ggplot(df_cov, aes(x = iteration, y = value, color = cov_element)) +
geom_line() +
labs(title = "Evolution of Covariance Matrix Elements",
x = "Iteration",
y = "Value") +
theme_minimal()
model$trace$mu
beta_estimated <- results$beta_estimated
beta_estimated <- do.call(rbind, beta_estimated)
beta_estimated <- t(beta_estimated)
plot(true_beta, col="red", type="b", xlab="Beta index", ylab="Beta value", main="Beta estimation")
lines(beta_estimated[,1], col="blue", type="b")
lines(beta_estimated[,2], col="blue", type="b")
lines(beta_estimated[,3], col="blue", type="b")
lines(beta_estimated[,4], col="blue", type="b")
mean_squared_errors <- apply(beta_estimated, 2, function(x) mean((x - true_beta)^2))
plot(training_sizes, mean_squared_errors, type="b", xlab="Beta index", ylab="Mean squared error", main="Mean squared error for beta estimation", log="x")
lines(x=training_sizes, rep(0, length(training_sizes)), col="red")
mean_abs_errors <- apply(beta_estimated, 2, function(x) mean(abs(x - true_beta)))
plot(training_sizes, mean_abs_errors, type="b", xlab="Beta index", ylab="Mean absolute error", main="Mean absolute error for beta estimation", log="x")
lines(x=training_sizes, rep(0, length(training_sizes)), col="red")
mean_squared_errors <- apply(mu_estimated, 2, function(x) mean((x - true_mu)^2))
plot(training_sizes, mean_squared_errors, type="b", xlab="Mu index", ylab="Mean squared error", main="Mean squared error for mu estimation", log="x")
lines(x=training_sizes, rep(0, length(training_sizes)), col="red")
mean_abs_errors <- apply(mu_estimated, 2, function(x) mean(abs(x - true_mu)))
plot(training_sizes, mean_abs_errors, type="b", xlab="Mu index", ylab="Mean absolute error", main="Mean absolute error for mu estimation", log="x")
lines(x=training_sizes, rep(0, length(training_sizes)), col="red")
Frobenius_norm_fun <- function(x, y) sqrt(sum((x - y)^2))
frobenius_norms <- c()
for (train_size in training_sizes){
frobenius_norms <- c(frobenius_norms, Frobenius_norm_fun(sigma_estimated_all[[as.character(train_size)]], true_sigma))
}
plot(training_sizes, frobenius_norms, type="b", xlab="Training size", ylab="Frobenius norm", main="Frobenius norm for sigma estimation", log="x")
lines(x=training_sizes, rep(0, length(training_sizes)), col="red")
library(reticulate)
library(dplyr)
library(stringr)
# remotes::install_local("~/INRIA/R_scripts/misaem_fork/", force=TRUE)
source("methods_in_R.R")
# reticulate::use_python(Sys.which("python3"))
# reticulate::use_python("C:\\Users\\Chris\\Anaconda3\\envs\\logistic\\python.exe")
exp <- "ExpC"
training_sizes <- c(100, 500, 1000, 5000, 10000, 50000, 100000, 200000, 300000)
test_size <- 15000
np <- import("numpy")
data <- np$load(file.path("data", exp, "original_data", "LOG_n315000_d3_corr065_prcNA035_prop105_rep0.npz"))
X_obs <- data$f[["X_obs"]]
M <- data$f[["M"]]
y <- data$f[["y"]]
data_test <- np$load(file.path("data", exp, "test_data", "LOG_n315000_d3_corr065_prcNA035_prop105_rep0.npz"))
X_test <- data_test$f[["X_obs"]]
M_test <- data_test$f[["M"]]
y_probs_test <- data_test$f[["y_probs"]]
y_test <- data_test$f[["y"]]
data_bayes <- np$load(file.path("data", exp, "bayes_data", "LOG_n315000_d3_corr065_prcNA035_prop105_rep0.npz"))
y_bayes <- data_bayes$f[["y_probs_bayes"]]
# Load the true beta
true_beta <- c(0, 1.62434536, -0.61175641, -0.52817175)
true_sigma <- toeplitz(c(1, 0.65, 0.65^2))
true_mu <- c(0, 0, 0)
n_train <- 1000
X_train <- X_obs[1:n_train,]
M_train <- M[1:n_train,]
print(sum(M_train)/length(M_train))
y_train <- y[1:n_train]
data_train <- as.data.frame(X_train)
data_train$y <- y_train
all_predictions <- list()
for (i in 1:1){
model <- misaem.fork::miss.glm("y ~ .", data=data_train, control=list(nmcmc=5, maxruns=20, k1=1, tau=9999),
init_params = list(beta=true_beta,
Sigma=true_sigma,
mu=true_mu))
y.hat <- predict(model, newdata = as.data.frame(X_test)[1:100,], mcmc_map=500)
all_predictions[[i]] <- y.hat
print(i)
}
y.hat <- do.call(cbind, all_predictions)
y.hat <- rowMeans(y.hat)
mean(abs(y.hat - y_bayes[1:100]))
model$trace$mu
mean(abs(y.hat[,1] - y_bayes[1:100]))
# Number of iterations
num_iter <- length(model$trace$X.iter)
# Compute covariance at each iteration
cov_evolution <- sapply(1:num_iter, function(k) {
Sigma <- cov(model$trace$X.iter[[k]])  # Compute covariance matrix
c(Sigma[1,1], Sigma[2,2], Sigma[3,3], Sigma[1,2], Sigma[2,3], Sigma[1,3])  # Extract elements
})
# Convert to a tidy dataframe for ggplot
df_cov <- data.frame(t(cov_evolution)) %>%
mutate(iteration = 1:num_iter) %>%
pivot_longer(cols = -iteration, names_to = "cov_element", values_to = "value")
# Rename elements for readability
df_cov$cov_element <- factor(df_cov$cov_element,
labels = c("Sigma11", "Sigma22", "Sigma33", "Sigma12", "Sigma23", "Sigma13"))
# Plot the evolution
ggplot(df_cov, aes(x = iteration, y = value, color = cov_element)) +
geom_line() +
labs(title = "Evolution of Covariance Matrix Elements",
x = "Iteration",
y = "Value") +
theme_minimal()
