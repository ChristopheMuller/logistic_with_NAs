{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def toep(d, rate):\n",
    "    return np.array([[(rate) ** abs(i - j) for i in range(d)] for j in range(d)])\n",
    "\n",
    "\n",
    "def generate_X(d, corr_rate, n):\n",
    "    \"\"\"Generate Gaussian feature matrix X with Toeplitz covariance.\"\"\"\n",
    "\n",
    "    cov = toep(d, corr_rate)\n",
    "    X = np.random.multivariate_normal(np.zeros(d), cov, size=n)\n",
    "    return X\n",
    "\n",
    "def compute_centricity_score(probs):\n",
    "    \"\"\"\n",
    "    Compute a more robust centricity score that better differentiates between distributions.\n",
    "    Returns a score between 0 and 1:\n",
    "    - Score near 0: probabilities concentrated near 0 and 1\n",
    "    - Score near 1: probabilities concentrated near 0.5\n",
    "    \"\"\"\n",
    "    # Calculate distances from 0.5\n",
    "    distances = np.abs(probs - 0.5)\n",
    "    \n",
    "    # Use a modified beta function that gives more weight to extreme values\n",
    "    beta_weight = np.power(1 - 2 * distances, 2)\n",
    "    \n",
    "    # Add penalty for asymmetric distributions\n",
    "    symmetry_penalty = np.abs(np.mean(probs > 0.5) - 0.5)\n",
    "    \n",
    "    # Combine scores with appropriate scaling\n",
    "    score = np.mean(beta_weight) * (1 - symmetry_penalty)\n",
    "    \n",
    "    return score\n",
    "\n",
    "def adjust_beta(X, target_score=0.5, score_threshold=0.001, max_iter=200, \n",
    "                learning_rate=0.1, patience=20):\n",
    "    \"\"\"\n",
    "    Adjust beta to achieve a desired distribution of probabilities p(y=1).\n",
    "    Uses a more robust optimization approach with adaptive steps.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Feature matrix (n x d)\n",
    "    - target_score: Desired centricity score (0 to 1)\n",
    "    - score_threshold: Threshold for stopping when close enough to target\n",
    "    - max_iter: Maximum iterations for adjustment\n",
    "    - learning_rate: Initial learning rate for beta adjustments\n",
    "    - patience: Number of iterations to wait before adjusting strategy\n",
    "    \n",
    "    Returns:\n",
    "    - beta: Adjusted coefficients\n",
    "    - score_history: History of scores during adjustment\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    \n",
    "    # Initialize beta based on target score\n",
    "    # Use smaller initial values for more extreme targets\n",
    "    initial_scale = np.exp(-2 * np.abs(target_score - 0.5))\n",
    "    beta = np.random.normal(0, initial_scale, size=d)\n",
    "    \n",
    "    score_history = []\n",
    "    best_score_diff = float('inf')\n",
    "    best_beta = beta.copy()\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # Compute probabilities\n",
    "        logits = X @ beta\n",
    "        probs = 1 / (1 + np.exp(-logits))\n",
    "        \n",
    "        # Compute score\n",
    "        score = compute_centricity_score(probs)\n",
    "        score_history.append(score)\n",
    "        \n",
    "        # Check if we're close enough to target\n",
    "        score_diff = np.abs(score - target_score)\n",
    "        if score_diff < score_threshold:\n",
    "            break\n",
    "            \n",
    "        # Update best solution if we've improved\n",
    "        if score_diff < best_score_diff:\n",
    "            best_score_diff = score_diff\n",
    "            best_beta = beta.copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Adaptive adjustment strategy\n",
    "        if patience_counter >= patience:\n",
    "            # Reset to best known solution and reduce learning rate\n",
    "            beta = best_beta.copy()\n",
    "            learning_rate *= 0.5\n",
    "            patience_counter = 0\n",
    "            \n",
    "        # Compute gradient-like direction\n",
    "        if score < target_score:\n",
    "            # Need more centered probabilities\n",
    "            beta *= (1 - learning_rate)\n",
    "        else:\n",
    "            # Need more extreme probabilities\n",
    "            beta *= (1 + learning_rate)\n",
    "            \n",
    "        # Add small random perturbation to avoid local optima\n",
    "        beta += np.random.normal(0, learning_rate * 0.01, size=d)\n",
    "    \n",
    "    # Return the best beta found\n",
    "    return best_beta, score_history\n",
    "\n",
    "def validate_distribution(X, beta, target_score, title=\"Probability Distribution\"):\n",
    "    \"\"\"\n",
    "    Validate and visualize the resulting probability distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Feature matrix\n",
    "    - beta: Coefficient vector\n",
    "    - target_score: Target centricity score\n",
    "    - title: Plot title\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Compute probabilities\n",
    "    logits = X @ beta\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    actual_score = compute_centricity_score(probs)\n",
    "    \n",
    "    # Create histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(probs, bins=40, density=True)\n",
    "    plt.title(f\"{title}\\nTarget Score: {target_score:.3f}, Actual Score: {actual_score:.3f}\")\n",
    "    plt.xlabel(\"Probability\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return actual_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of setups =  9\n",
      "REP 0\n",
      "REP 1\n",
      "REP 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the percentages of missingness and number of replicates\n",
    "experiment_name = \"ExpB\"\n",
    "experiment_data_folder = os.path.join(\"data\", experiment_name)\n",
    "\n",
    "# Create necessary directories\n",
    "for subdir in [\"original_data\", \"test_data\", \"pred_data\", \"bayes_data\"]:\n",
    "    path = os.path.join(experiment_data_folder, subdir)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# Modified parameters\n",
    "missingness_percentages = [0.35]\n",
    "n_replicates = 3\n",
    "Ds = [5]\n",
    "Corrs = [0.33]\n",
    "CentricityScores = [0.20, 0.35, 0.85]  # Replaced PropOfY1 with CentricityScores\n",
    "n_train = 10000\n",
    "n_test = 15000\n",
    "n = n_train + n_test\n",
    "\n",
    "print(\"# of setups = \", n_replicates * len(missingness_percentages) * len(Corrs) * len(CentricityScores))\n",
    "\n",
    "N_MC = 100\n",
    "\n",
    "# Modified set-up dataframe\n",
    "df_set_up = pd.DataFrame({\n",
    "    \"rep\": [],\n",
    "    \"n\": [],\n",
    "    \"true_beta\": [],\n",
    "    \"center_X\": [],\n",
    "    \"set_up\": [],\n",
    "    \"d\": [],\n",
    "    \"corr\": [],\n",
    "    \"prcNA\": [],\n",
    "    \"centricity\": [],\n",
    "}).T\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "def generate_mask(n, d, prc):\n",
    "    \"\"\"Generate missing data mask.\"\"\"\n",
    "    return np.random.binomial(n=1, p=prc, size=(n, d))\n",
    "\n",
    "def sigma(x):\n",
    "    \"\"\"Sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "for rep in range(n_replicates):\n",
    "    print(\"REP\", rep)\n",
    "    for d in Ds:\n",
    "        for corr in Corrs:\n",
    "            corr_str = str(corr).replace(\".\", \"\")\n",
    "            \n",
    "            for centricity in CentricityScores:\n",
    "                centricity_str = str(centricity).replace(\".\", \"\")\n",
    "\n",
    "                # Generate X using the provided function\n",
    "                X_full = generate_X(d=d, corr_rate=corr, n=n)\n",
    "                center_X = np.zeros(d)  # Since X is already centered in generate_X\n",
    "\n",
    "                # Adjust beta to achieve desired centricity score\n",
    "                beta0, score_history = adjust_beta(X_full, target_score=centricity)\n",
    "                \n",
    "                # Generate probabilities and outcomes\n",
    "                y_probs = sigma(X_full @ beta0)\n",
    "                y = np.random.binomial(n=1, p=y_probs)\n",
    "                final_centricity = 4 * np.mean(y_probs * (1 - y_probs))\n",
    "\n",
    "                for prc in missingness_percentages:\n",
    "                    prc_str = str(prc).replace(\".\", \"\")\n",
    "                    set_up = f\"LOG_n{n}_d{d}_corr{corr_str}_prcNA{prc_str}_cent{centricity_str}_rep{rep}\"\n",
    "\n",
    "                    M = generate_mask(n, d, prc)\n",
    "                    \n",
    "                    X_obs = X_full.copy()\n",
    "                    X_obs[M == 1] = np.nan\n",
    "\n",
    "                    new_row = pd.Series({\n",
    "                        \"rep\": rep,\n",
    "                        \"n\": n,\n",
    "                        \"d\": d,\n",
    "                        \"corr\": corr,\n",
    "                        \"prcNA\": prc,\n",
    "                        \"centricity\": centricity,\n",
    "                        \"true_beta\": beta0,\n",
    "                        \"center_X\": center_X,\n",
    "                        \"set_up\": set_up\n",
    "                    })\n",
    "\n",
    "                    df_set_up = pd.concat([df_set_up, new_row], axis=1, ignore_index=True)\n",
    "\n",
    "                    # Save original data\n",
    "                    data_to_save = {\n",
    "                        \"X_obs\": X_obs,\n",
    "                        \"M\": M,\n",
    "                        \"y\": y,\n",
    "                        \"y_probs\": y_probs,\n",
    "                        \"X_full\": X_full\n",
    "                    }\n",
    "                    np.savez(os.path.join(experiment_data_folder, \"original_data\", f\"{set_up}.npz\"), **data_to_save)\n",
    "\n",
    "                    # Save test data\n",
    "                    X_test = X_obs[n_train:]\n",
    "                    y_test = y[n_train:]\n",
    "                    y_probs_test = y_probs[n_train:]\n",
    "                    M_test = M[n_train:]\n",
    "                    data_to_save_test = {\n",
    "                        \"X_obs\": X_test,\n",
    "                        \"M\": M_test,\n",
    "                        \"y\": y_test,\n",
    "                        \"y_probs\": y_probs_test,\n",
    "                        \"X_full\": X_full[n_train:]\n",
    "                    }\n",
    "                    np.savez(os.path.join(experiment_data_folder, \"test_data\", f\"{set_up}.npz\"), **data_to_save_test)\n",
    "\n",
    "                    # Calculate and save Bayes predictions\n",
    "                    def toep(d, rho):\n",
    "                        \"\"\"Generate Toeplitz correlation matrix.\"\"\"\n",
    "                        return np.fromfunction(lambda i, j: rho ** np.abs(i - j), (d, d))\n",
    "\n",
    "                    def get_y_prob_bayes(X_test, full_mu, full_cov, true_beta, n_mc):\n",
    "                        \"\"\"Calculate Bayesian predictions.\"\"\"\n",
    "                        n, d = X_test.shape\n",
    "                        y_probs = np.zeros((n, n_mc))\n",
    "                        \n",
    "                        for i in range(n):\n",
    "                            miss_idx = np.isnan(X_test[i])\n",
    "                            if np.any(miss_idx):\n",
    "                                X_samples = np.random.multivariate_normal(\n",
    "                                    full_mu, full_cov, size=n_mc\n",
    "                                )\n",
    "                                X_samples[:, ~miss_idx] = X_test[i, ~miss_idx]\n",
    "                                y_probs[i] = sigma(X_samples @ true_beta)\n",
    "                            else:\n",
    "                                y_probs[i] = sigma(X_test[i] @ true_beta)\n",
    "                        \n",
    "                        return y_probs\n",
    "\n",
    "                    y_probs_bayes = get_y_prob_bayes(X_test, full_mu=center_X, \n",
    "                                                    full_cov=toep(d, corr), \n",
    "                                                    true_beta=beta0, n_mc=N_MC)\n",
    "                    y_probs_bayes = y_probs_bayes.mean(axis=1)\n",
    "\n",
    "                    data_to_save_bayes = {\n",
    "                        \"y_probs_bayes\": y_probs_bayes\n",
    "                    }\n",
    "                    np.savez(os.path.join(experiment_data_folder, \"bayes_data\", f\"{set_up}.npz\"), **data_to_save_bayes)\n",
    "\n",
    "# Save setup dataframe\n",
    "df_set_up.T.to_csv(os.path.join(experiment_data_folder, \"set_up.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logistic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
